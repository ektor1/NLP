{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Companies Dataset with Google & FastText Vectors\n",
    "This notebook includes the code applied to articles regarding companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once cell - then restart kernel\n",
    "!pip install --upgrade tensorflow \n",
    "!pip install gensim \n",
    "!pip install -q -U keras-tuner\n",
    "!pip install scikeras[tensorflow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy, binary_crossentropy\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "import keras_tuner as kt\n",
    "from scikeras.wrappers import KerasClassifier # To use keras with sklearn \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  \n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Model33s - sklearn\n",
    "import pickle # to save the models \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (recall_score, precision_score, precision_recall_curve,\n",
    "                             fbeta_score, make_scorer)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight \n",
    "\n",
    "# gensim \n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Load nltk library for tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Garbage collector\n",
    "import gc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Company adverse media train, test, and validation datasets\n",
    "org_train = pd.read_csv(\"datasets/Org_train.csv\")\n",
    "org_test = pd.read_csv(\"datasets/Org_test.csv\")\n",
    "org_valid = pd.read_csv(\"datasets/Org_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "Removing columns containing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop unspecified columns that appear to be irrelevant in the datasets\n",
    "org_train.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "org_test.drop(['Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1'], axis=1, inplace=True)\n",
    "org_valid.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization\n",
    "Tokenize text in train, test, and validation sets. Lower capitalized letters, remove stopwords and non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "from func_ import preprocess\n",
    "train_corpus = [preprocess(potato) for potato in org_train['raw_sentence']]\n",
    "test_corpus = [preprocess(text) for text in org_test['raw_sentence']]\n",
    "valid_corpus = [preprocess(text) for text in org_valid['raw_sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store tokenized sentences in new column called 'token_sentence'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [org_train, org_test, org_valid] \n",
    "corpus = [train_corpus, test_corpus, valid_corpus]\n",
    "\n",
    "for (df,corpus) in zip(datasets,corpus):\n",
    "    df['token_sentence'] = pd.NaT # Create new column to store tokenized sentences\n",
    "    tok_column = df.pop('token_sentence')\n",
    "    df.insert(0,'token_sentence', tok_column) # Shift column to first position in df\n",
    "    df['token_sentence'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for deep learning\n",
    "y_train = pd.DataFrame(org_train['class'])\n",
    "y_valid = pd.DataFrame(org_valid['class'])\n",
    "y_test = pd.DataFrame(org_test['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for deep learning\n",
    "for labels in [y_train, y_valid, y_test]:\n",
    "    for index, row in labels.itertuples():\n",
    "        if row == 'negative':\n",
    "            labels.loc[index] = 0\n",
    "        else:\n",
    "            labels.loc[index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for types of text in the df for ease of use - for deep learning\n",
    "X_train = org_train['token_sentence']\n",
    "X_train_raw = org_train['raw_sentence']\n",
    "y_train = y_train['class']\n",
    "\n",
    "X_test = org_test['token_sentence']\n",
    "X_test_raw = org_test['raw_sentence']\n",
    "y_test = y_test['class']\n",
    "\n",
    "X_valid = org_valid['token_sentence']\n",
    "X_valid_raw = org_valid['raw_sentence']\n",
    "y_valid = y_valid['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for types of text in the df for ease of use\n",
    "X_train = org_train['token_sentence']\n",
    "X_train_raw = org_train['raw_sentence']\n",
    "y_train = org_train['class']\n",
    "\n",
    "X_test = org_test['token_sentence']\n",
    "X_test_raw = org_test['raw_sentence']\n",
    "y_test = org_test['class']\n",
    "\n",
    "X_valid = org_valid['token_sentence']\n",
    "X_valid_raw = org_valid['raw_sentence']\n",
    "y_valid = org_valid['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google News pre-trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'datasets/GoogleNews-vectors-negative300.bin.gz', binary=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'datasets/GoogleNews-vectors-negative300.bin.gz', binary=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Data Cleaning\n",
    "Remove rows that contain less than 2 words. That is rows with only 1 or 0 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import filter_docs\n",
    "trainsets = [X_train, X_test, X_valid]\n",
    "testsets = [y_train, y_test, y_valid]\n",
    "filter_docs(trainsets, testsets, lambda text: (len(text)<2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove sentences that are not included in the google vecs dictionary. These are sentences that include only words that are not found in the Google News pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from func_ import filter_docs, has_vector_representation\n",
    "filter_docs(trainsets, testsets, lambda text: has_vector_representation(Model, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain average vectors for each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Average vectors for each sentence\n",
    "from func_ import average_vecs\n",
    "trainVecs = [average_vecs(sentence, Model, 300) for sentence in X_train]\n",
    "testVecs = [average_vecs(sentence, Model, 300) for sentence in X_test]\n",
    "validVecs = [average_vecs(sentence, Model, 300) for sentence in X_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imbalance\n",
    "Find how imbalanced our datasets are between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of instances for each class in each dataset\n",
    "datasets={'Train Set':y_train, 'Test Set':y_test, 'Valid Set':y_valid}\n",
    "\n",
    "for dataset, data in datasets.items():\n",
    "    labels, counts = np.unique(data, return_counts=True)\n",
    "    neg = counts[0]\n",
    "    pos = counts[1]\n",
    "    total = neg + pos\n",
    "    print('{} Examples:\\n    Negative: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(dataset,\n",
    "        neg, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Apply stemming on each sentence. Then calculate their average vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import stemming\n",
    "stem_train = [stemming(sentence) for sentence in X_train]\n",
    "stem_test = [stemming(sentence) for sentence in X_test]\n",
    "stem_valid = [stemming(sentence) for sentence in X_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import average_vecs\n",
    "trainVecs = [average_vecs(sentence, Model, 300) for sentence in stem_train]\n",
    "testVecs = [average_vecs(sentence, Model, 300) for sentence in stem_test]\n",
    "validVecs = [average_vecs(sentence, Model, 300) for sentence in stem_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Apply lemmatization on each sentence. Then Calculate their average vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import lemmatization \n",
    "lem_train = [lemmatization(sentence) for sentence in X_train]\n",
    "lem_test = [lemmatization(sentence) for sentence in X_test]\n",
    "lem_valid = [lemmatization(sentence) for sentence in X_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import average_vecs\n",
    "trainVecs = [average_vecs(sentence, Model, 300) for sentence in lem_train]\n",
    "testVecs = [average_vecs(sentence, Model, 300) for sentence in lem_test]\n",
    "validVecs = [average_vecs(sentence, Model, 300) for sentence in lem_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "Apply CountVectorizer() and TfidfVectorizer() with DecisionTreeClassifier & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english')\n",
    "train_countvec = vec.fit_transform(X_train_raw)\n",
    "test_countvec = vec.transform(X_test_raw)\n",
    "valid_countvec = vec.transform(X_valid_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree with Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import classifiers_gs\n",
    "model_list = [DecisionTreeClassifier()]\n",
    "model_names = ['DTC_countvec_rgs']\n",
    "\n",
    "grids = [{\n",
    "    'random_state': [42],\n",
    "    'max_depth': range(5,1000,5),\n",
    "    'max_features': ['sqrt','log2'],\n",
    "    'min_samples_leaf': range(1,36),\n",
    "    'min_samples_split': range(1,26)\n",
    "    }]\n",
    "\n",
    "results, best_models, timer = classifiers_gs(model_list, model_names, grids, train_countvec, test_countvec, valid_countvec, y_train, y_test, y_valid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Grid Search\n",
    "Applying 9 specific combinations of parameters as the model takes too long to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import classifiers_gs_svm\n",
    "# Default parameters of SVM are C: 1.0, gamma: scale  \n",
    "model = SVC(probability=True)\n",
    "model_name = \"SVM_countvec_rgs_\"\n",
    "\n",
    "grids = [{'gamma': [4]},\n",
    "         {'gamma': [6]},\n",
    "         {'gamma': [8]},\n",
    "         {'C': [20]},\n",
    "         {'C': [50]},\n",
    "         {'C': [100]},\n",
    "         {'gamma': [4], 'C': [20]},\n",
    "         {'gamma': [6], 'C': [50]},\n",
    "         {'gamma': [8], 'C': [100]}]\n",
    "\n",
    "\n",
    "results = classifiers_gs_svm(model, model_name, grids, train_countvec, test_countvec, valid_countvec, y_train, y_test, y_valid)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import countvec\n",
    "vec = TfidfVectorizer(stop_words='english')\n",
    "model_list = [DecisionTreeClassifier, SVC]\n",
    "countvec(vec, model_list, X_train_raw,X_test_raw,X_valid_raw,y_train,y_test,y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Vectorizer\n",
    "Apply HashingVectorizer() with DecisionTreeClassifier. Then compare recall_score on test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import hashvec\n",
    "model_list = [DecisionTreeClassifier, SVC]\n",
    "hashvec(model_list, X_train_raw,X_test_raw,X_valid_raw,y_train,y_test,y_valid,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain Models\n",
    "\n",
    "### Decision Tree Classifier & SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import classifiers\n",
    "model_list = [DecisionTreeClassifier(random_state=42)]\n",
    "model_names = ['DecisionTreeClassifier']\n",
    "\n",
    "results, precision_test, recall_test, precision_valid, recall_valid = classifiers(model_list, model_names, trainVecs, testVecs, validVecs, y_train, y_test, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import classifiers\n",
    "model_list = [SVC(random_state=42, probability=True)]\n",
    "model_names = ['SVM']\n",
    "\n",
    "results, precision_test, recall_test, precision_valid, recall_valid = classifiers(model_list, model_names, trainVecs, testVecs, validVecs, y_train, y_test, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier with random grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import classifiers_gs\n",
    "model_list = [DecisionTreeClassifier]\n",
    "model_names = ['DecisionTreeClassifier']\n",
    "\n",
    "grids = [{\n",
    "    'random_state': [42],\n",
    "    'max_depth': range(5,1000,5),\n",
    "    'max_features': ['sqrt','log2', None],\n",
    "    'min_samples_leaf': range(1,36),\n",
    "    'min_samples_split': range(1,26)\n",
    "    }]\n",
    "\n",
    "results, best_models, timer = classifiers_gs(model_list, model_names, grids, trainVecs, testVecs, validVecs, y_train, y_test, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Grid Search\n",
    "Applying 7 specific combinations of parameters as the model takes too long to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from func_ import svm_grid_search\n",
    "model_name = \"SVM_gs_lem\"\n",
    "\n",
    "grids = [\n",
    "    {'gamma': 0.1, 'C': 10.0},\n",
    "    {'gamma': 0.2, 'C': 20.0},\n",
    "    {'gamma': 0.3, 'C': 30.0},\n",
    "    {'gamma': 0.4, 'C': 40.0},\n",
    "    {'gamma': 0.5, 'C': 50.0},\n",
    "    {'gamma': 0.6, 'C': 70.0},\n",
    "    {'gamma': 0.7, 'C': 100.0}]\n",
    "\n",
    "results = svm_grid_search(trainVecs, validVecs, y_train, y_valid, model_name, grids)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results_gs_plain.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to arrays\n",
    "Xtrain = np.stack(trainVecs)\n",
    "Xvalid = np.stack(validVecs)\n",
    "Xtest = np.stack(testVecs)\n",
    "y_train = np.array(y_train).astype('float32')\n",
    "y_valid = np.array(y_valid).astype('float32')\n",
    "y_test = np.array(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "neg, pos = counts[0], counts[1] # get number of positive and negative values in training set\n",
    "output_bias = np.log([pos/neg])\n",
    "output_bias # correct initial bias according to: https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_train), y=y_train)\n",
    "class_weight = {0: class_weight[0], 1: class_weight[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from func_ import build_gs_nn\n",
    "params_grid = dict(\n",
    "    layers = [2, 3, 4],\n",
    "    neurons = [200, 300, 400, 500],\n",
    "    learning_rate = [1e-2, 1e-3],\n",
    "    dropout_rates = [0.5, 0.6, 0.7],\n",
    "    epochs = [2, 5, 8, 10],\n",
    "    batch_sizes = [350, 450, 600, 1000]\n",
    "    ) # 1152 models\n",
    "\n",
    "results_lem = build_gs_nn(Xtrain, y_train, Xvalid, y_valid, Xtest, y_test, params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lem.to_csv('results_lem.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_lem.sort_values(by='99%', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_lem['Average'] = pd.NaT\n",
    "for n_row, row in enumerate(results_lem):\n",
    "    results_lem['Average'].iloc[n_row] = (results_lem['90%'].iloc[n_row]*0.05 + results_lem['95%'].iloc[n_row]*0.05 + results_lem['99%'].iloc[n_row]*0.9)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_lem.sort_values(by=\"Average\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_scores = [0.90, 0.95, 0.99]\n",
    "for recall in recall_scores:\n",
    "    for row, score in enumerate(recall_test):\n",
    "        if round(score, 2) == recall:\n",
    "            print(f\"Precision at {recall}% is {precision_test[row]}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"results.csv\")\n",
    "results.sort_values(by='99%', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(index=range(1), columns=[\"90%\", \"95%\", \"99%\"])\n",
    "recall_scores = [0.90, 0.95, 0.99]\n",
    "counter=0\n",
    "for recall in recall_scores:\n",
    "    for row, score in enumerate(recall_test):\n",
    "        if round(score, 2) == recall:\n",
    "            performance.loc[counter, str(int(recall*100))+\"%\"] = precision_test[row]\n",
    "            break\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters of a model\n",
    "model_filename='SVM_avg_model.sav'\n",
    "model = pickle.load(open(model_filename, 'rb'))\n",
    "model.get_params()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
